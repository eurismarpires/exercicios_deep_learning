{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from collections import OrderedDict\n",
    "from math import ceil\n",
    "from tensorflow.contrib.session_bundle import exporter\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from util.gpu import get_available_gpus\n",
    "from util.log import merge_logs\n",
    "from util.spell import correction\n",
    "from util.shared_lib import check_cupti\n",
    "from util.text import sparse_tensor_value_to_texts, wer\n",
    "from xdg import BaseDirectory as xdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ds_importer = os.environ.get('ds_importer', 'ldc93s1')\n",
    "ds_dataset_path = os.environ.get('ds_dataset_path', os.path.join('./data', ds_importer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "ds_importer_module = importlib.import_module('util.importers.%s' % ds_importer)\n",
    "\n",
    "from util.website import maybe_publish\n",
    "\n",
    "do_fulltrace = bool(len(os.environ.get('ds_do_fulltrace', '')))\n",
    "\n",
    "if do_fulltrace:\n",
    "    check_cupti()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Global Constants\n",
    "# ================\n",
    "\n",
    "# The number of iterations (epochs) we will train for\n",
    "epochs = int(os.environ.get('ds_epochs', 100))\n",
    "\n",
    "# Whether to use GPU bound Warp-CTC\n",
    "use_warpctc = bool(len(os.environ.get('ds_use_warpctc', '')))\n",
    "\n",
    "# As we will employ dropout on the feedforward layers of the network,\n",
    "# we need to define a parameter `dropout_rate` that keeps track of the dropout rate for these layers\n",
    "dropout_rate = float(os.environ.get('ds_dropout_rate', 0.05))  # TODO: Validate this is a reasonable value\n",
    "\n",
    "# We allow for customisation of dropout per-layer\n",
    "dropout_rate2 = float(os.environ.get('ds_dropout_rate2', dropout_rate))\n",
    "dropout_rate3 = float(os.environ.get('ds_dropout_rate3', dropout_rate))\n",
    "dropout_rate4 = float(os.environ.get('ds_dropout_rate4', 0.0))\n",
    "dropout_rate5 = float(os.environ.get('ds_dropout_rate5', 0.0))\n",
    "dropout_rate6 = float(os.environ.get('ds_dropout_rate6', dropout_rate))\n",
    "\n",
    "dropout_rates = [ dropout_rate,\n",
    "                  dropout_rate2,\n",
    "                  dropout_rate3,\n",
    "                  dropout_rate4,\n",
    "                  dropout_rate5,\n",
    "                  dropout_rate6 ]\n",
    "no_dropout = [ 0.0 ] * 6\n",
    "\n",
    "# One more constant required of the non-recurrant layers is the clipping value of the ReLU.\n",
    "relu_clip = int(os.environ.get('ds_relu_clip', 20)) # TODO: Validate this is a reasonable value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Adam optimizer (http://arxiv.org/abs/1412.6980) parameters\n",
    "\n",
    "# Beta 1 parameter\n",
    "beta1 = float(os.environ.get('ds_beta1', 0.9)) # TODO: Determine a reasonable value for this\n",
    "\n",
    "# Beta 2 parameter\n",
    "beta2 = float(os.environ.get('ds_beta2', 0.999)) # TODO: Determine a reasonable value for this\n",
    "\n",
    "# Epsilon parameter\n",
    "epsilon = float(os.environ.get('ds_epsilon', 1e-8)) # TODO: Determine a reasonable value for this\n",
    "\n",
    "# Learning rate parameter\n",
    "learning_rate = float(os.environ.get('ds_learning_rate', 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Batch sizes\n",
    "\n",
    "# The number of elements in a training batch\n",
    "train_batch_size = int(os.environ.get('ds_train_batch_size', 1))\n",
    "\n",
    "# The number of elements in a dev batch\n",
    "dev_batch_size = int(os.environ.get('ds_dev_batch_size', 1))\n",
    "\n",
    "# The number of elements in a test batch\n",
    "test_batch_size = int(os.environ.get('ds_test_batch_size', 1))\n",
    "\n",
    "\n",
    "# Sample limits\n",
    "\n",
    "# The maximum amount of samples taken from (the beginning of) the train set - 0 meaning no limit\n",
    "limit_train = int(os.environ.get('ds_limit_train', 0))\n",
    "\n",
    "# The maximum amount of samples taken from (the beginning of) the validation set - 0 meaning no limit\n",
    "limit_dev   = int(os.environ.get('ds_limit_dev',   0))\n",
    "\n",
    "# The maximum amount of samples taken from (the beginning of) the test set - 0 meaning no limit\n",
    "limit_test  = int(os.environ.get('ds_limit_test',  0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step widths\n",
    "\n",
    "# The number of epochs we cycle through before displaying progress\n",
    "display_step = int(os.environ.get('ds_display_step', 1))\n",
    "\n",
    "# The number of epochs we cycle through before checkpointing the model\n",
    "checkpoint_step = int(os.environ.get('ds_checkpoint_step', 5))\n",
    "\n",
    "# The number of epochs we cycle through before validating the model\n",
    "validation_step = int(os.environ.get('ds_validation_step', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Checkpointing\n",
    "\n",
    "# The directory in which checkpoints are stored\n",
    "checkpoint_dir = os.environ.get('ds_checkpoint_dir', xdg.save_data_path('deepspeech'))\n",
    "\n",
    "# Whether to resume from checkpoints when training\n",
    "restore_checkpoint = bool(int(os.environ.get('ds_restore_checkpoint', 0)))\n",
    "\n",
    "# The directory in which exported models are stored\n",
    "export_dir = os.environ.get('ds_export_dir', None)\n",
    "\n",
    "# The version number of the exported model\n",
    "export_version = 1\n",
    "\n",
    "# Whether to remove old exported models\n",
    "remove_export = bool(int(os.environ.get('ds_remove_export', 0)))\n",
    "\n",
    "\n",
    "# Reporting\n",
    "\n",
    "# The number of phrases to print out during a WER report\n",
    "report_count = int(os.environ.get('ds_report_count', 10))\n",
    "\n",
    "# Whether to log device placement of the operators to the console\n",
    "log_device_placement = bool(int(os.environ.get('ds_log_device_placement', 0)))\n",
    "\n",
    "# Whether to log gradients and variables summaries to TensorBoard during training.\n",
    "# This incurs a performance hit, so should generally only be enabled for debugging.\n",
    "log_variables = bool(len(os.environ.get('ds_log_variables', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Geometry\n",
    "\n",
    "# The layer width to use when initialising layers\n",
    "n_hidden = int(os.environ.get('ds_n_hidden', 494))\n",
    "\n",
    "\n",
    "# Initialization\n",
    "\n",
    "# The default random seed that is used to initialize variables. Ensures reproducibility.\n",
    "random_seed = int(os.environ.get('ds_random_seed', 4567)) # To be adjusted in case of bad luck\n",
    "\n",
    "# The default standard deviation to use when initialising weights and biases\n",
    "default_stddev = float(os.environ.get('ds_default_stddev', 0.046875))\n",
    "\n",
    "# Individual standard deviations to use when initialising particular weights and biases\n",
    "for var in ['b1', 'h1', 'b2', 'h2', 'b3', 'h3', 'b5', 'h5', 'b6', 'h6']:\n",
    "    locals()['%s_stddev' % var] = float(os.environ.get('ds_%s_stddev' % var, default_stddev))\n",
    "\n",
    "# Session settings\n",
    "\n",
    "# Standard session configuration that'll be used for all new sessions.\n",
    "session_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=log_device_placement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Geometric Constants\n",
    "# ===================\n",
    "\n",
    "# For an explanation of the meaning of the geometric constants, please refer to\n",
    "# doc/Geometry.md\n",
    "\n",
    "# Number of MFCC features\n",
    "n_input = 26 # TODO: Determine this programatically from the sample rate\n",
    "\n",
    "# The number of frames in the context\n",
    "n_context = 9 # TODO: Determine the optimal value using a validation data set\n",
    "\n",
    "# Number of units in hidden layers\n",
    "n_hidden_1 = n_hidden\n",
    "n_hidden_2 = n_hidden\n",
    "n_hidden_5 = n_hidden\n",
    "\n",
    "# LSTM cell state dimension\n",
    "n_cell_dim = n_hidden\n",
    "\n",
    "# The number of units in the third layer, which feeds in to the LSTM\n",
    "n_hidden_3 = 2 * n_cell_dim\n",
    "\n",
    "# The number of characters in the target language plus one\n",
    "n_character = 29 # TODO: Determine if this should be extended with other punctuation\n",
    "\n",
    "# The number of units in the sixth layer\n",
    "n_hidden_6 = n_character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Graph Creation\n",
    "# ==============\n",
    "\n",
    "def variable_on_cpu(name, shape, initializer):\n",
    "    r\"\"\"\n",
    "    Next we concern ourselves with graph creation.\n",
    "    However, before we do so we must introduce a utility function ``variable_on_cpu()``\n",
    "    used to create a variable in CPU memory.\n",
    "    \"\"\"\n",
    "    # Use the /cpu:0 device for scoped operations\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Create or get apropos variable\n",
    "        var = tf.get_variable(name=name, shape=shape, initializer=initializer)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def BiRNN(batch_x, seq_length, dropout):\n",
    "    r\"\"\"\n",
    "    That done, we will define the learned variables, the weights and biases,\n",
    "    within the method ``BiRNN()`` which also constructs the neural network.\n",
    "    The variables named ``hn``, where ``n`` is an integer, hold the learned weight variables.\n",
    "    The variables named ``bn``, where ``n`` is an integer, hold the learned bias variables.\n",
    "    In particular, the first variable ``h1`` holds the learned weight matrix that\n",
    "    converts an input vector of dimension ``n_input + 2*n_input*n_context``\n",
    "    to a vector of dimension ``n_hidden_1``.\n",
    "    Similarly, the second variable ``h2`` holds the weight matrix converting\n",
    "    an input vector of dimension ``n_hidden_1`` to one of dimension ``n_hidden_2``.\n",
    "    The variables ``h3``, ``h5``, and ``h6`` are similar.\n",
    "    Likewise, the biases, ``b1``, ``b2``..., hold the biases for the various layers.\n",
    "    \"\"\"\n",
    "    # Input shape: [batch_size, n_steps, n_input + 2*n_input*n_context]\n",
    "    batch_x_shape = tf.shape(batch_x)\n",
    "\n",
    "    # Reshaping `batch_x` to a tensor with shape `[n_steps*batch_size, n_input + 2*n_input*n_context]`.\n",
    "    # This is done to prepare the batch for input into the first layer which expects a tensor of rank `2`.\n",
    "\n",
    "    # Permute n_steps and batch_size\n",
    "    batch_x = tf.transpose(batch_x, [1, 0, 2])\n",
    "    # Reshape to prepare input for first layer\n",
    "    batch_x = tf.reshape(batch_x, [-1, n_input + 2*n_input*n_context]) # (n_steps*batch_size, n_input + 2*n_input*n_context)\n",
    "\n",
    "    # The next three blocks will pass `batch_x` through three hidden layers with\n",
    "    # clipped RELU activation and dropout.\n",
    "\n",
    "    # 1st layer\n",
    "    b1 = variable_on_cpu('b1', [n_hidden_1], tf.random_normal_initializer(stddev=b1_stddev))\n",
    "    h1 = variable_on_cpu('h1', [n_input + 2*n_input*n_context, n_hidden_1], tf.random_normal_initializer(stddev=h1_stddev))\n",
    "    layer_1 = tf.minimum(tf.nn.relu(tf.add(tf.matmul(batch_x, h1), b1)), relu_clip)\n",
    "    layer_1 = tf.nn.dropout(layer_1, (1.0 - dropout[0]))\n",
    "\n",
    "    # 2nd layer\n",
    "    b2 = variable_on_cpu('b2', [n_hidden_2], tf.random_normal_initializer(stddev=b2_stddev))\n",
    "    h2 = variable_on_cpu('h2', [n_hidden_1, n_hidden_2], tf.random_normal_initializer(stddev=h2_stddev))\n",
    "    layer_2 = tf.minimum(tf.nn.relu(tf.add(tf.matmul(layer_1, h2), b2)), relu_clip)\n",
    "    layer_2 = tf.nn.dropout(layer_2, (1.0 - dropout[1]))\n",
    "\n",
    "    # 3rd layer\n",
    "    b3 = variable_on_cpu('b3', [n_hidden_3], tf.random_normal_initializer(stddev=b3_stddev))\n",
    "    h3 = variable_on_cpu('h3', [n_hidden_2, n_hidden_3], tf.random_normal_initializer(stddev=h3_stddev))\n",
    "    layer_3 = tf.minimum(tf.nn.relu(tf.add(tf.matmul(layer_2, h3), b3)), relu_clip)\n",
    "    layer_3 = tf.nn.dropout(layer_3, (1.0 - dropout[2]))\n",
    "\n",
    "    # Now we create the forward and backward LSTM units.\n",
    "    # Both of which have inputs of length `n_cell_dim` and bias `1.0` for the forget gate of the LSTM.\n",
    "\n",
    "    # Forward direction cell:\n",
    "    lstm_fw_cell = tf.contrib.rnn.BasicLSTMCell(n_cell_dim, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_fw_cell = tf.contrib.rnn.DropoutWrapper(lstm_fw_cell,\n",
    "                                                 input_keep_prob=1.0 - dropout[3],\n",
    "                                                 output_keep_prob=1.0 - dropout[3],\n",
    "                                                 seed=random_seed)\n",
    "    # Backward direction cell:\n",
    "    lstm_bw_cell = tf.contrib.rnn.BasicLSTMCell(n_cell_dim, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_bw_cell = tf.contrib.rnn.DropoutWrapper(lstm_bw_cell,\n",
    "                                                 input_keep_prob=1.0 - dropout[4],\n",
    "                                                 output_keep_prob=1.0 - dropout[4],\n",
    "                                                 seed=random_seed)\n",
    "\n",
    "    # `layer_3` is now reshaped into `[n_steps, batch_size, 2*n_cell_dim]`,\n",
    "    # as the LSTM BRNN expects its input to be of shape `[max_time, batch_size, input_size]`.\n",
    "    layer_3 = tf.reshape(layer_3, [-1, batch_x_shape[0], n_hidden_3])\n",
    "\n",
    "    # Now we feed `layer_3` into the LSTM BRNN cell and obtain the LSTM BRNN output.\n",
    "    outputs, output_states = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw_cell,\n",
    "                                                             cell_bw=lstm_bw_cell,\n",
    "                                                             inputs=layer_3,\n",
    "                                                             dtype=tf.float32,\n",
    "                                                             time_major=True,\n",
    "                                                             sequence_length=seq_length)\n",
    "\n",
    "    # Reshape outputs from two tensors each of shape [n_steps, batch_size, n_cell_dim]\n",
    "    # to a single tensor of shape [n_steps*batch_size, 2*n_cell_dim]\n",
    "    outputs = tf.concat(outputs, 2)\n",
    "    outputs = tf.reshape(outputs, [-1, 2*n_cell_dim])\n",
    "\n",
    "    # Now we feed `outputs` to the fifth hidden layer with clipped RELU activation and dropout\n",
    "    b5 = variable_on_cpu('b5', [n_hidden_5], tf.random_normal_initializer(stddev=b5_stddev))\n",
    "    h5 = variable_on_cpu('h5', [(2 * n_cell_dim), n_hidden_5], tf.random_normal_initializer(stddev=h5_stddev))\n",
    "    layer_5 = tf.minimum(tf.nn.relu(tf.add(tf.matmul(outputs, h5), b5)), relu_clip)\n",
    "    layer_5 = tf.nn.dropout(layer_5, (1.0 - dropout[5]))\n",
    "\n",
    "    # Now we apply the weight matrix `h6` and bias `b6` to the output of `layer_5`\n",
    "    # creating `n_classes` dimensional vectors, the logits.\n",
    "    b6 = variable_on_cpu('b6', [n_hidden_6], tf.random_normal_initializer(stddev=b6_stddev))\n",
    "    h6 = variable_on_cpu('h6', [n_hidden_5, n_hidden_6], tf.random_normal_initializer(stddev=h6_stddev))\n",
    "    layer_6 = tf.add(tf.matmul(layer_5, h6), b6)\n",
    "\n",
    "    # Finally we reshape layer_6 from a tensor of shape [n_steps*batch_size, n_hidden_6]\n",
    "    # to the slightly more useful shape [n_steps, batch_size, n_hidden_6].\n",
    "    # Note, that this differs from the input in that it is time-major.\n",
    "    layer_6 = tf.reshape(layer_6, [-1, batch_x_shape[0], n_hidden_6])\n",
    "\n",
    "    # Output shape: [n_steps, batch_size, n_hidden_6]\n",
    "    return layer_6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Accuracy and Loss\n",
    "# =================\n",
    "\n",
    "# In accord with \"Deep Speech: Scaling up end-to-end speech recognition\"\n",
    "# (http://arxiv.org/abs/1412.5567),\n",
    "# the loss function used by our network should be the CTC loss function\n",
    "# (http://www.cs.toronto.edu/~graves/preprint.pdf).\n",
    "# Conveniently, this loss function is implemented in TensorFlow.\n",
    "# Thus, we can simply make use of this implementation to define our loss.\n",
    "\n",
    "def calculate_accuracy_and_loss(batch_set, dropout):\n",
    "    r\"\"\"\n",
    "    This routine beam search decodes a mini-batch and calculates the loss and accuracy.\n",
    "    Next to total and average loss it returns the accuracy,\n",
    "    the decoded result and the batch's original Y.\n",
    "    \"\"\"\n",
    "    # Obtain the next batch of data\n",
    "    batch_x, batch_seq_len, batch_y = batch_set.next_batch()\n",
    "\n",
    "    # Calculate the logits of the batch using BiRNN\n",
    "    logits = BiRNN(batch_x, tf.to_int64(batch_seq_len), dropout)\n",
    "\n",
    "    # Compute the CTC loss using either TensorFlow's `ctc_loss` or Baidu's `warp_ctc_loss`.\n",
    "    if use_warpctc:\n",
    "        total_loss = tf.contrib.warpctc.warp_ctc_loss(labels=batch_y, inputs=logits, sequence_length=batch_seq_len)\n",
    "    else:\n",
    "        total_loss = tf.nn.ctc_loss(labels=batch_y, inputs=logits, sequence_length=batch_seq_len)\n",
    "\n",
    "    # Calculate the average loss across the batch\n",
    "    avg_loss = tf.reduce_mean(total_loss)\n",
    "\n",
    "    # Beam search decode the batch\n",
    "    decoded, _ = tf.nn.ctc_beam_search_decoder(logits, batch_seq_len, merge_repeated=False)\n",
    "\n",
    "    # Compute the edit (Levenshtein) distance\n",
    "    distance = tf.edit_distance(tf.cast(decoded[0], tf.int32), batch_y)\n",
    "\n",
    "    # Compute the accuracy\n",
    "    accuracy = tf.reduce_mean(distance)\n",
    "\n",
    "    # Finally we return the\n",
    "    # - calculated total and\n",
    "    # - average losses,\n",
    "    # - the Levenshtein distance,\n",
    "    # - the recognition accuracy,\n",
    "    # - the decoded batch and\n",
    "    # - the original batch_y (which contains the verified transcriptions).\n",
    "    return total_loss, avg_loss, distance, accuracy, decoded, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Adam Optimization\n",
    "# =================\n",
    "\n",
    "# In constrast to \"Deep Speech: Scaling up end-to-end speech recognition\"\n",
    "# (http://arxiv.org/abs/1412.5567),\n",
    "# in which \"Nesterov's Accelerated Gradient Descent\"\n",
    "# (www.cs.toronto.edu/~fritz/absps/momentum.pdf) was used,\n",
    "# we will use the Adam method for optimization (http://arxiv.org/abs/1412.6980),\n",
    "# because, generally, it requires less fine-tuning.\n",
    "def create_optimizer():\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                       beta1=beta1,\n",
    "                                       beta2=beta2,\n",
    "                                       epsilon=epsilon)\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Towers\n",
    "# ======\n",
    "\n",
    "# In order to properly make use of multiple GPU's, one must introduce new abstractions,\n",
    "# not present when using a single GPU, that facilitate the multi-GPU use case.\n",
    "# In particular, one must introduce a means to isolate the inference and gradient\n",
    "# calculations on the various GPU's.\n",
    "# The abstraction we intoduce for this purpose is called a 'tower'.\n",
    "# A tower is specified by two properties:\n",
    "# * **Scope** - A scope, as provided by `tf.name_scope()`,\n",
    "# is a means to isolate the operations within a tower.\n",
    "# For example, all operations within \"tower 0\" could have their name prefixed with `tower_0/`.\n",
    "# * **Device** - A hardware device, as provided by `tf.device()`,\n",
    "# on which all operations within the tower execute.\n",
    "# For example, all operations of \"tower 0\" could execute on the first GPU `tf.device('/gpu:0')`.\n",
    "\n",
    "# As we are introducing one tower for each GPU, first we must determine how many GPU's are available\n",
    "# Get a list of the available gpu's ['/gpu:0', '/gpu:1'...]\n",
    "available_devices = get_available_gpus()\n",
    "\n",
    "# If there are no GPU's use the CPU\n",
    "if 0 == len(available_devices):\n",
    "    available_devices = ['/cpu:0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_tower_results(batch_set, optimizer=None):\n",
    "    r\"\"\"\n",
    "    With this preliminary step out of the way, we can for each GPU introduce a\n",
    "    tower for which's batch we calculate\n",
    "\n",
    "    * The CTC decodings ``decoded``,\n",
    "    * The (total) loss against the outcome (Y) ``total_loss``,\n",
    "    * The loss averaged over the whole batch ``avg_loss``,\n",
    "    * The optimization gradient (computed based on the averaged loss),\n",
    "    * The Levenshtein distances between the decodings and their transcriptions ``distance``,\n",
    "    * The accuracy of the outcome averaged over the whole batch ``accuracy``\n",
    "\n",
    "    and retain the original ``labels`` (Y).\n",
    "    ``decoded``, ``labels``, the optimization gradient, ``distance``, ``accuracy``,\n",
    "    ``total_loss`` and ``avg_loss`` are collected into the corresponding arrays\n",
    "    ``tower_decodings``, ``tower_labels``, ``tower_gradients``, ``tower_distances``,\n",
    "    ``tower_accuracies``, ``tower_total_losses``, ``tower_avg_losses`` (dimension 0 being the tower).\n",
    "    Finally this new method ``get_tower_results()`` will return those tower arrays.\n",
    "    In case of ``tower_accuracies`` and ``tower_avg_losses``, it will return the\n",
    "    averaged values instead of the arrays.\n",
    "    \"\"\"\n",
    "    # Tower labels to return\n",
    "    tower_labels = []\n",
    "\n",
    "    # Tower decodings to return\n",
    "    tower_decodings = []\n",
    "\n",
    "    # Tower distances to return\n",
    "    tower_distances = []\n",
    "\n",
    "    # Tower total batch losses to return\n",
    "    tower_total_losses = []\n",
    "\n",
    "    # Tower gradients to return\n",
    "    tower_gradients = []\n",
    "\n",
    "    # To calculate the mean of the accuracies\n",
    "    tower_accuracies = []\n",
    "\n",
    "    # To calculate the mean of the losses\n",
    "    tower_avg_losses = []\n",
    "\n",
    "    with tf.variable_scope(tf.get_variable_scope()):\n",
    "        # Loop over available_devices\n",
    "        for i in xrange(len(available_devices)):\n",
    "            # Execute operations of tower i on device i\n",
    "            with tf.device(available_devices[i]):\n",
    "                # Create a scope for all operations of tower i\n",
    "                with tf.name_scope('tower_%d' % i) as scope:\n",
    "                    # Calculate the avg_loss and accuracy and retrieve the decoded\n",
    "                    # batch along with the original batch's labels (Y) of this tower\n",
    "                    total_loss, avg_loss, distance, accuracy, decoded, labels = \\\n",
    "                        calculate_accuracy_and_loss(batch_set, no_dropout if optimizer is None else dropout_rates)\n",
    "\n",
    "                    # Allow for variables to be re-used by the next tower\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                    # Retain tower's labels (Y)\n",
    "                    tower_labels.append(labels)\n",
    "\n",
    "                    # Retain tower's decoded batch\n",
    "                    tower_decodings.append(decoded)\n",
    "\n",
    "                    # Retain tower's distances\n",
    "                    tower_distances.append(distance)\n",
    "\n",
    "                    # Retain tower's total losses\n",
    "                    tower_total_losses.append(total_loss)\n",
    "\n",
    "                    if optimizer is not None:\n",
    "                        # Compute gradients for model parameters using tower's mini-batch\n",
    "                        gradients = optimizer.compute_gradients(avg_loss)\n",
    "\n",
    "                        # Retain tower's gradients\n",
    "                        tower_gradients.append(gradients)\n",
    "\n",
    "                    # Retain tower's accuracy\n",
    "                    tower_accuracies.append(accuracy)\n",
    "\n",
    "                    # Retain tower's avg losses\n",
    "                    tower_avg_losses.append(avg_loss)\n",
    "\n",
    "    # Return the results tuple, the gradients, and the means of accuracies and losses\n",
    "    return (tower_labels, tower_decodings, tower_distances, tower_total_losses), \\\n",
    "           tower_gradients, \\\n",
    "           tf.reduce_mean(tower_accuracies, 0), \\\n",
    "           tf.reduce_mean(tower_avg_losses, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def average_gradients(tower_gradients):\n",
    "    r\"\"\"\n",
    "    A routine for computing each variable's average of the gradients obtained from the GPUs.\n",
    "    Note also that this code acts as a syncronization point as it requires all\n",
    "    GPUs to be finished with their mini-batch before it can run to completion.\n",
    "    \"\"\"\n",
    "    # List of average gradients to return to the caller\n",
    "    average_grads = []\n",
    "\n",
    "    # Loop over gradient/variable pairs from all towers\n",
    "    for grad_and_vars in zip(*tower_gradients):\n",
    "        # Introduce grads to store the gradients for the current variable\n",
    "        grads = []\n",
    "\n",
    "        # Loop over the gradients for the current variable\n",
    "        for g, _ in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension\n",
    "        grad = tf.concat(grads, 0)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Create a gradient/variable tuple for the current variable with its average gradient\n",
    "        grad_and_var = (grad, grad_and_vars[0][1])\n",
    "\n",
    "        # Add the current tuple to average_grads\n",
    "        average_grads.append(grad_and_var)\n",
    "\n",
    "    # Return result to caller\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def apply_gradients(optimizer, average_grads):\n",
    "    r\"\"\"\n",
    "    Now next we introduce a function to apply the averaged gradients to update the\n",
    "    model's paramaters on the CPU\n",
    "    \"\"\"\n",
    "    apply_gradient_op = optimizer.apply_gradients(average_grads)\n",
    "    return apply_gradient_op\n",
    "\n",
    "\n",
    "# Logging\n",
    "# =======\n",
    "\n",
    "def log_variable(variable, gradient=None):\n",
    "    r\"\"\"\n",
    "    We introduce a function for logging a tensor variable's current state.\n",
    "    It logs scalar values for the mean, standard deviation, minimum and maximum.\n",
    "    Furthermore it logs a histogram of its state and (if given) of an optimization gradient.\n",
    "    \"\"\"\n",
    "    name = variable.name\n",
    "    mean = tf.reduce_mean(variable)\n",
    "    tf.summary.scalar(name='%s/mean'   % name, tensor=mean)\n",
    "    tf.summary.scalar(name='%s/sttdev' % name, tensor=tf.sqrt(tf.reduce_mean(tf.square(variable - mean))))\n",
    "    tf.summary.scalar(name='%s/max'    % name, tensor=tf.reduce_max(variable))\n",
    "    tf.summary.scalar(name='%s/min'    % name, tensor=tf.reduce_min(variable))\n",
    "    tf.summary.histogram(name=name, values=variable)\n",
    "    if gradient is not None:\n",
    "        if isinstance(gradient, tf.IndexedSlices):\n",
    "            grad_values = gradient.values\n",
    "        else:\n",
    "            grad_values = gradient\n",
    "        if grad_values is not None:\n",
    "            tf.summary.histogram(name='%s/gradients' % name, values=grad_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def log_grads_and_vars(grads_and_vars):\n",
    "    r\"\"\"\n",
    "    Let's also introduce a helper function for logging collections of gradient/variable tuples.\n",
    "    \"\"\"\n",
    "    for gradient, variable in grads_and_vars:\n",
    "        log_variable(variable, gradient=gradient)\n",
    "\n",
    "\n",
    "# Finally we define the top directory for all logs and our current log sub-directory of it.\n",
    "# We also add some log helpers.\n",
    "logs_dir = os.environ.get('ds_logs_dir', 'logs')\n",
    "log_dir = '%s/%s' % (logs_dir, time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "def get_git_revision_hash():\n",
    "    return subprocess.check_output(['git', 'rev-parse', 'HEAD']).strip()\n",
    "\n",
    "def get_git_branch():\n",
    "    return subprocess.check_output(['git', 'rev-parse', '--abbrev-ref', 'HEAD']).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Helpers\n",
    "# =======\n",
    "\n",
    "def calculate_and_print_wer_report(caption, results_tuple):\n",
    "    r\"\"\"\n",
    "    This routine will print a WER report with a given caption.\n",
    "    It'll print the `mean` WER plus summaries of the ``ds_report_count`` top lowest\n",
    "    loss items from the provided WER results tuple\n",
    "    (only items with WER!=0 and ordered by their WER).\n",
    "    \"\"\"\n",
    "    items = zip(*results_tuple)\n",
    "\n",
    "    count = len(items)\n",
    "    mean_wer = 0.0\n",
    "    for i in xrange(count):\n",
    "        item = items[i]\n",
    "        # If distance > 0 we know that there is a WER > 0 and have to calculate it\n",
    "        if item[2] > 0:\n",
    "            # Replace result by language model corrected result\n",
    "            item = (item[0], correction(item[1]), item[2], item[3])\n",
    "            # Replacing accuracy tuple entry by the WER\n",
    "            item = (item[0], item[1], wer(item[0], item[1]), item[3])\n",
    "            # Replace items[i] with new item\n",
    "            items[i] = item\n",
    "            mean_wer = mean_wer + item[2]\n",
    "\n",
    "    # Getting the mean WER from the accumulated one\n",
    "    mean_wer = mean_wer / float(count)\n",
    "\n",
    "    # Filter out all items with WER=0\n",
    "    items = [a for a in items if a[2] > 0]\n",
    "\n",
    "    # Order the remaining items by their loss (lowest loss on top)\n",
    "    items.sort(key=lambda a: a[3])\n",
    "\n",
    "    # Take only the first report_count items\n",
    "    items = items[:report_count]\n",
    "\n",
    "    # Order this top ten items by their WER (lowest WER on top)\n",
    "    items.sort(key=lambda a: a[2])\n",
    "\n",
    "    print \"%s WER: %f\" % (caption, mean_wer)\n",
    "    for a in items:\n",
    "        print \"-\" * 80\n",
    "        print \"    WER:    %f\" % a[2]\n",
    "        print \"    loss:   %f\" % a[3]\n",
    "        print \"    source: \\\"%s\\\"\" % a[0]\n",
    "        print \"    result: \\\"%s\\\"\" % a[1]\n",
    "\n",
    "    return mean_wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def collect_results(results_tuple, returns):\n",
    "    r\"\"\"\n",
    "    This routine will help collecting partial results for the WER reports.\n",
    "    The ``results_tuple`` is composed of an array of the original labels,\n",
    "    an array of the corresponding decodings, an array of the corrsponding\n",
    "    distances and an array of the corresponding losses. ``returns`` is built up\n",
    "    in a similar way, containing just the unprocessed results of one\n",
    "    ``session.run`` call (effectively of one batch).\n",
    "    Labels and decodings are converted to text before splicing them into their\n",
    "    corresponding results_tuple lists. In the case of decodings,\n",
    "    for now we just pick the first available path.\n",
    "    \"\"\"\n",
    "    # Each of the arrays within results_tuple will get extended by a batch of each available device\n",
    "    for i in xrange(len(available_devices)):\n",
    "        # Collect the labels\n",
    "        results_tuple[0].extend(sparse_tensor_value_to_texts(returns[0][i]))\n",
    "\n",
    "        # Collect the decodings - at the moment we default to the first one\n",
    "        results_tuple[1].extend(sparse_tensor_value_to_texts(returns[1][i][0]))\n",
    "\n",
    "        # Collect the distances\n",
    "        results_tuple[2].extend(returns[2][i])\n",
    "\n",
    "        # Collect the losses\n",
    "        results_tuple[3].extend(returns[3][i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# For reporting we also need a standard way to do time measurements.\n",
    "def stopwatch(start_duration=0):\n",
    "    r\"\"\"\n",
    "    This function will toggle a stopwatch.\n",
    "    The first call starts it, second call stops it, third call continues it etc.\n",
    "    So if you want to measure the accumulated time spent in a certain area of the code,\n",
    "    you can surround that code by stopwatch-calls like this:\n",
    "\n",
    "    .. code:: python\n",
    "\n",
    "        fun_time = 0 # initializes a stopwatch\n",
    "        [...]\n",
    "        for i in xrange(10):\n",
    "          [...]\n",
    "          # Starts/continues the stopwatch - fun_time is now a point in time (again)\n",
    "          fun_time = stopwatch(fun_time)\n",
    "          fun()\n",
    "          # Pauses the stopwatch - fun_time is now a duration\n",
    "          fun_time = stopwatch(fun_time)\n",
    "        [...]\n",
    "        # The following line only makes sense after an even call of :code:`fun_time = stopwatch(fun_time)`.\n",
    "        print \"Time spent in fun():\", format_duration(fun_time)\n",
    "\n",
    "    \"\"\"\n",
    "    if start_duration == 0:\n",
    "        return datetime.datetime.utcnow()\n",
    "    else:\n",
    "        return datetime.datetime.utcnow() - start_duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def format_duration(duration):\n",
    "    \"\"\"Formats the result of an even stopwatch call as hours:minutes:seconds\"\"\"\n",
    "    m, s = divmod(duration.seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return \"%d:%02d:%02d\" % (h, m, s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Execution\n",
    "# =========\n",
    "\n",
    "\n",
    "# To run our different graphs in separate sessions,\n",
    "# we first need to create some common infrastructure.\n",
    "#\n",
    "# At first we introduce the functions `read_data_sets` and `read_data_set` to read in data sets.\n",
    "# The first returns a `DataSets` object of the selected importer, containing all available sets.\n",
    "# The latter takes the name of the required data set\n",
    "# (`'train'`, `'dev'` or `'test'`) as string and returns the respective set.\n",
    "def read_data_sets(set_names):\n",
    "    r\"\"\"\n",
    "    Returns a :class:`DataSets` object of the selected importer, containing all available sets.\n",
    "    \"\"\"\n",
    "    # Obtain all the data sets\n",
    "    return ds_importer_module.read_data_sets(ds_dataset_path,\n",
    "                                             train_batch_size,\n",
    "                                             dev_batch_size,\n",
    "                                             test_batch_size,\n",
    "                                             n_input,\n",
    "                                             n_context,\n",
    "                                             limit_dev=limit_dev,\n",
    "                                             limit_test=limit_test,\n",
    "                                             limit_train=limit_train,\n",
    "                                             sets=set_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_data_set(set_name):\n",
    "    r\"\"\"\n",
    "    ``set_name``: string, the name of the required data set (``'train'``, ``'dev'`` or ``'test'``)\n",
    "\n",
    "    Returns the respective set.\n",
    "    \"\"\"\n",
    "    # Obtain all the data sets\n",
    "    data_sets = read_data_sets([set_name])\n",
    "\n",
    "    # Pick the train, dev, or test data set from it\n",
    "    return getattr(data_sets, set_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_execution_context(set_name):\n",
    "    r\"\"\"\n",
    "    The most important data structure that will be shared among the following\n",
    "    routines is a so called ``execution context``.\n",
    "    It's a tuple with four elements: The graph, the data set (one of train/dev/test),\n",
    "    the top level graph entry point tuple from ``get_tower_results()``\n",
    "    and a saver object for persistence.\n",
    "\n",
    "    Let's at first introduce the construction routine for an execution context.\n",
    "    It takes the data set's name as string (\"train\", \"dev\" or \"test\")\n",
    "    and returns the execution context tuple.\n",
    "\n",
    "    An execution context tuple is of the form ``(graph, data_set, tower_results, saver)``\n",
    "    when not training. ``graph`` is the ``tf.Graph`` in which the operators reside.\n",
    "    ``data_set`` is the selected data set (train, dev, or test).\n",
    "    ``tower_results`` is the result of a call to ``get_tower_results()``.\n",
    "    ``saver`` is a ``tf.train.Saver`` which can be used to save the model.\n",
    "\n",
    "    When training an execution context is of the form\n",
    "    ``(graph, data_set, tower_results, saver, apply_gradient_op, merged, writer)``.\n",
    "    The first four items are the same as in the above case.\n",
    "    ``apply_gradient_op`` is an operator that applies the gradents to the learned parameters.\n",
    "    ``merged`` contains all summaries for tensorboard.\n",
    "    Finally, ``writer`` is the ``tf.train.SummaryWriter`` used to write summaries for tensorboard.\n",
    "    \"\"\"\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Set the global random seed for determinism\n",
    "        tf.set_random_seed(random_seed)\n",
    "\n",
    "        # Get the required data set\n",
    "        data_set = read_data_set(set_name)\n",
    "\n",
    "        # Define bool to indicate if data_set is the training set\n",
    "        is_train = set_name == 'train'\n",
    "\n",
    "        # If training create optimizer\n",
    "        optimizer = create_optimizer() if is_train else None\n",
    "\n",
    "        # Get the data_set specific graph end-points\n",
    "        tower_results = get_tower_results(data_set, optimizer=optimizer)\n",
    "\n",
    "        if is_train:\n",
    "            # Average tower gradients\n",
    "            avg_tower_gradients = average_gradients(tower_results[1])\n",
    "\n",
    "            # Add logging of averaged gradients\n",
    "            if log_variables:\n",
    "                log_grads_and_vars(avg_tower_gradients)\n",
    "\n",
    "            # Apply gradients to modify the model\n",
    "            apply_gradient_op = apply_gradients(optimizer, avg_tower_gradients)\n",
    "\n",
    "        # Create a saver to checkpoint the model\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "        if is_train:\n",
    "            # Prepare tensor board logging\n",
    "            merged = tf.summary.merge_all()\n",
    "            writer = tf.summary.FileWriter(log_dir, graph)\n",
    "            return (graph, data_set, tower_results, saver, apply_gradient_op, merged, writer)\n",
    "        else:\n",
    "            return (graph, data_set, tower_results, saver)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def start_execution_context(execution_context, model_path=None):\n",
    "    r\"\"\"\n",
    "    Now let's introduce a routine for starting an execution context.\n",
    "    By passing in the execution context and the file path of the model, it will\n",
    "\n",
    "    1. Create a new session using the execution model's graph,\n",
    "    2. Load (restore) the model from the file path into it,\n",
    "    3. Start the associated queue and runner threads.\n",
    "\n",
    "    Finally it will return the new session.\n",
    "    \"\"\"\n",
    "    # Obtain the Graph in which to execute\n",
    "    graph = execution_context[0]\n",
    "\n",
    "    # Create a new session and load the execution context's graph into it\n",
    "    session = tf.Session(config=session_config, graph=graph)\n",
    "\n",
    "    # Set graph as the default Graph\n",
    "    with graph.as_default():\n",
    "        if model_path is None:\n",
    "            # Init all variables for first use\n",
    "            init_op = tf.global_variables_initializer()\n",
    "            session.run(init_op)\n",
    "        else:\n",
    "            # Loading the model into the session\n",
    "            execution_context[3].restore(session, model_path)\n",
    "\n",
    "        # Create Coordinator to manage threads\n",
    "        coord = tf.train.Coordinator()\n",
    "\n",
    "        # Start queue runner threads\n",
    "        managed_threads = tf.train.start_queue_runners(sess=session, coord=coord)\n",
    "\n",
    "        # Start importer's queue threads\n",
    "        managed_threads = managed_threads + execution_context[1].start_queue_threads(session, coord)\n",
    "\n",
    "    return session, coord, managed_threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def persist_model(execution_context, session, checkpoint_path, global_step):\n",
    "    r\"\"\"\n",
    "    This helper method persists the contained model to disk and returns\n",
    "    the model's filename, constructed from ``checkpoint_path`` and ``global_step``\n",
    "    \"\"\"\n",
    "    # Saving session's model into checkpoint dir\n",
    "    return execution_context[3].save(session, checkpoint_path, global_step=global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def stop_execution_context(execution_context, session, coord, managed_threads, checkpoint_path=None, global_step=None):\n",
    "    r\"\"\"\n",
    "    The following helper method stops an execution context.\n",
    "    Before closing the provided ``session``, it will persist the contained model to disk.\n",
    "    The model's filename will be returned.\n",
    "    \"\"\"\n",
    "    # If the model is not persisted, we'll return 'None'\n",
    "    hibernation_path = None\n",
    "\n",
    "    if checkpoint_path is not None and global_step is not None:\n",
    "        # Saving session's model into checkpoint dir\n",
    "        hibernation_path = persist_model(execution_context, session, checkpoint_path, global_step)\n",
    "\n",
    "    # Close importer's queue\n",
    "    execution_context[1].close_queue(session)\n",
    "\n",
    "    # Request managed threads stop\n",
    "    coord.request_stop()\n",
    "\n",
    "    # Wait until managed threads stop\n",
    "    coord.join(managed_threads)\n",
    "\n",
    "    # Free all allocated resources of the session\n",
    "    session.close()\n",
    "\n",
    "    return hibernation_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss_and_report(execution_context, session, epoch=-1, query_report=False):\n",
    "    r\"\"\"\n",
    "    Now let's introduce the main routine for training and inference.\n",
    "    It takes a started execution context (given by ``execution_context``),\n",
    "    a ``Session`` (``session``), an optional epoch index (``epoch``)\n",
    "    and a flag (``query_report``) which indicates whether to calculate the WER\n",
    "    report data or not.\n",
    "    Its main duty is to iterate over all batches and calculate the mean loss.\n",
    "    If a non-negative epoch is provided, it will also optimize the parameters.\n",
    "    If ``query_report`` is ``False``, the default, it will return a tuple which\n",
    "    contains the mean loss.\n",
    "    If ``query_report`` is ``True``, the mean accuracy and individual results\n",
    "    are also included in the returned tuple.\n",
    "    \"\"\"\n",
    "    # An epoch of -1 means no train run\n",
    "    do_training = epoch >= 0\n",
    "\n",
    "    # Unpack variables\n",
    "    if do_training:\n",
    "        graph, data_set, tower_results, saver, apply_gradient_op, merged, writer = execution_context\n",
    "    else:\n",
    "        graph, data_set, tower_results, saver = execution_context\n",
    "    results_params, _, avg_accuracy, avg_loss = tower_results\n",
    "\n",
    "    batches_per_device = ceil(float(data_set.total_batches) / len(available_devices))\n",
    "\n",
    "    total_loss = 0.0\n",
    "    params = OrderedDict()\n",
    "    params['avg_loss'] = avg_loss\n",
    "\n",
    "    # Facilitate a train run\n",
    "    if do_training:\n",
    "        params['apply_gradient_op'] = apply_gradient_op\n",
    "        if log_variables:\n",
    "            params['merged'] = merged\n",
    "\n",
    "    # Requirements to display a WER report\n",
    "    if query_report:\n",
    "        # Reset accuracy\n",
    "        total_accuracy = 0.0\n",
    "        # Create report results tuple\n",
    "        report_results = ([],[],[],[])\n",
    "        # Extend the session.run parameters\n",
    "        params['sample_results'] = [results_params, avg_accuracy]\n",
    "\n",
    "    # Get the index of each of the session fetches so we can recover the results more easily\n",
    "    param_idx = dict(zip(params.keys(), range(len(params))))\n",
    "    params = params.values()\n",
    "\n",
    "    # Loop over the batches\n",
    "    for batch in range(int(batches_per_device)):\n",
    "        extra_params = { }\n",
    "        if do_training and do_fulltrace:\n",
    "            loss_run_metadata            = tf.RunMetadata()\n",
    "            extra_params['options']      = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            extra_params['run_metadata'] = loss_run_metadata\n",
    "\n",
    "        # Compute the batch\n",
    "        result = session.run(params, **extra_params)\n",
    "\n",
    "        # Add batch to loss\n",
    "        total_loss += result[param_idx['avg_loss']]\n",
    "\n",
    "        if do_training:\n",
    "            # Log all variable states in current step\n",
    "            step = epoch * data_set.total_batches + batch * len(available_devices)\n",
    "            if log_variables:\n",
    "                writer.add_summary(result[param_idx['merged']], step)\n",
    "            if do_fulltrace:\n",
    "                writer.add_run_metadata(loss_run_metadata, 'loss_epoch%d_batch%d'   % (epoch, batch))\n",
    "            writer.flush()\n",
    "\n",
    "        if query_report:\n",
    "            sample_results = result[param_idx['sample_results']]\n",
    "            # Collect individual sample results\n",
    "            collect_results(report_results, sample_results[0])\n",
    "            # Add batch to total_accuracy\n",
    "            total_accuracy += sample_results[1]\n",
    "\n",
    "    # Returning the batch set result tuple\n",
    "    loss = total_loss / batches_per_device\n",
    "    if query_report:\n",
    "        return (loss, total_accuracy / batches_per_device, report_results)\n",
    "    else:\n",
    "        return (loss, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_report(caption, batch_set_result):\n",
    "    r\"\"\"\n",
    "    This routine will print a report from a provided batch set result tuple.\n",
    "    It takes a caption for titling the output plus the batch set result tuple.\n",
    "    If the batch set result tuple contains accuracy and a report results tuple,\n",
    "    a complete WER report will be calculated, printed and its mean WER returned.\n",
    "    Otherwise it will just print the loss and return ``None``.\n",
    "    \"\"\"\n",
    "    # Unpacking batch set result tuple\n",
    "    loss, accuracy, results_tuple = batch_set_result\n",
    "\n",
    "    # We always have a loss value\n",
    "    title = caption + \" loss=\" + \"{:.9f}\".format(loss)\n",
    "\n",
    "    mean_wer = None\n",
    "    if accuracy is not None and results_tuple is not None:\n",
    "        title += \" avg_cer=\" + \"{:.9f}\".format(accuracy)\n",
    "        mean_wer = calculate_and_print_wer_report(title, results_tuple)\n",
    "    else:\n",
    "        print title\n",
    "\n",
    "    return mean_wer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run_set(set_name, model_path=None, query_report=False):\n",
    "    r\"\"\"\n",
    "    Let's also introduce a routine that facilitates obtaining results from a data set\n",
    "    (given by its name in ``set_name``) - from execution context creation to closing the session.\n",
    "    If a model's filename is provided by ``model_path``,\n",
    "    it will initialize the session by loading the given model into it.\n",
    "    It will return the loss and - if ``query_report=True`` - also the accuracy and the report results tuple.\n",
    "    \"\"\"\n",
    "    # Creating the execution context\n",
    "    execution_context = create_execution_context(set_name)\n",
    "\n",
    "    # Starting the execution context\n",
    "    session, coord, managed_threads = start_execution_context(execution_context, model_path=model_path)\n",
    "\n",
    "    # Applying the batches\n",
    "    batch_set_result = calculate_loss_and_report(execution_context, session, query_report=query_report)\n",
    "\n",
    "    # Cleaning up\n",
    "    stop_execution_context(execution_context, session, coord, managed_threads)\n",
    "\n",
    "    # Returning batch_set_result from calculate_loss_and_report\n",
    "    return batch_set_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING Optimization\n",
      "\n",
      "STARTING Epoch 0000\n",
      "Training model...\n",
      "Training loss=792.634765625 avg_cer=3.657142878 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   791.058533\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"ygyqyqyqyqyqyqygyiqyqylqlyqyqyqyqyqyqlylqylylylygygygygyqylygygygyglgygygylyqyqygygygygyglqyqyqyqlyqlylyqyqyglqlylylyqlqlqyqlylyl\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   794.211060\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"ygyqyqyqyqyqyqyqyqyqyqylylylyqyqyqlqlylylylylylyqgygygygylylylygyglgygygylylqyqygygygygygygqlqyqyqyqylylylylqylqyglyqyqlyiyqyqylyl\"\n",
      "FINISHED Epoch 0000   Overall epoch time: 0:00:43   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0001\n",
      "Training model...\n",
      "Training loss=197.221099854 avg_cer=1.000000000 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   197.007874\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   197.434341\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "FINISHED Epoch 0001   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0002\n",
      "Training model...\n",
      "Training loss=114.456253052 avg_cer=0.942857146 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   114.380890\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"aa\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   114.531616\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"aa\"\n",
      "FINISHED Epoch 0002   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0003\n",
      "Training model...\n",
      "Training loss=199.846298218 avg_cer=0.771428585 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   199.304749\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"oaoaoaoaooao aaaaaoaoaoaoaoa\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   200.387848\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o oaoaoaoaoaoaoaaoaoaooaoaoa\"\n",
      "FINISHED Epoch 0003   Overall epoch time: 0:00:01   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0004\n",
      "Training model...\n",
      "Training loss=103.292877197 avg_cer=0.942857146 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   103.193253\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   103.392494\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "Hibernating training session into directory /home/eurismar/.local/share/deepspeech\n",
      "FINISHED Epoch 0004   Overall epoch time: 0:00:01   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0005\n",
      "Training model...\n",
      "Training loss=132.004135132 avg_cer=0.971428573 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   131.129730\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   132.878540\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "FINISHED Epoch 0005   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0006\n",
      "Training model...\n",
      "Training loss=135.683074951 avg_cer=0.971428573 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   135.451111\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   135.915039\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "FINISHED Epoch 0006   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0007\n",
      "Training model...\n",
      "Training loss=118.972717285 avg_cer=0.971428573 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   118.937805\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   119.007629\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "FINISHED Epoch 0007   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0008\n",
      "Training model...\n",
      "Training loss=98.685363770 avg_cer=0.885714293 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   98.652245\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   98.718491\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "FINISHED Epoch 0008   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0009\n",
      "Training model...\n",
      "Training loss=96.227508545 avg_cer=0.857142866 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   95.986252\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   96.468758\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "Hibernating training session into directory /home/eurismar/.local/share/deepspeech\n",
      "FINISHED Epoch 0009   Overall epoch time: 0:00:01   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0010\n",
      "Training model...\n",
      "Training loss=110.640884399 avg_cer=0.828571439 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   110.434937\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"u u r\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   110.846840\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "FINISHED Epoch 0010   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0011\n",
      "Training model...\n",
      "Training loss=104.638656616 avg_cer=0.800000012 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   104.408691\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"or r\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   104.868629\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"or r\"\n",
      "FINISHED Epoch 0011   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0012\n",
      "Training model...\n",
      "Training loss=93.446037292 avg_cer=0.885714293 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   93.410980\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"r\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   93.481094\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o\"\n",
      "FINISHED Epoch 0012   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0013\n",
      "Training model...\n",
      "Training loss=92.680709839 avg_cer=0.914285719 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   92.664963\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"too\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   92.696465\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"too\"\n",
      "FINISHED Epoch 0013   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0014\n",
      "Training model...\n",
      "Training loss=97.387176514 avg_cer=0.928571463 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   97.304459\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"of\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   97.469894\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"too\"\n",
      "Hibernating training session into directory /home/eurismar/.local/share/deepspeech\n",
      "FINISHED Epoch 0014   Overall epoch time: 0:00:01   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0015\n",
      "Training model...\n",
      "Training loss=100.272125244 avg_cer=0.942857146 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   100.179497\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"of\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   100.364746\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"of\"\n",
      "FINISHED Epoch 0015   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0016\n",
      "Training model...\n",
      "Training loss=98.740478516 avg_cer=0.914285719 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   98.712204\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"too\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   98.768745\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"too\"\n",
      "FINISHED Epoch 0016   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0017\n",
      "Training model...\n",
      "Training loss=94.604080200 avg_cer=0.914285719 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   94.565849\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"too\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   94.642303\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"too\"\n",
      "FINISHED Epoch 0017   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0018\n",
      "Training model...\n",
      "Training loss=92.148818970 avg_cer=0.885714293 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   92.045013\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"ooooh\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   92.252632\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"ooooh\"\n",
      "FINISHED Epoch 0018   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0019\n",
      "Training model...\n",
      "Training loss=92.330970764 avg_cer=0.871428609 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   92.222244\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"ooooh\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   92.439697\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"voodoo\"\n",
      "Hibernating training session into directory /home/eurismar/.local/share/deepspeech\n",
      "FINISHED Epoch 0019   Overall epoch time: 0:00:01   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0020\n",
      "Training model...\n",
      "Training loss=94.024162292 avg_cer=0.871428609 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   93.827469\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"voodoo\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   94.220856\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"ooooh\"\n",
      "FINISHED Epoch 0020   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0021\n",
      "Training model...\n",
      "Training loss=93.930664062 avg_cer=0.885714293 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   93.586777\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o as\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   94.274551\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"too\"\n",
      "FINISHED Epoch 0021   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0022\n",
      "Training model...\n",
      "Training loss=92.022018433 avg_cer=0.899999976 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   91.944511\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   92.099533\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"of\"\n",
      "FINISHED Epoch 0022   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0023\n",
      "Training model...\n",
      "Training loss=90.234115601 avg_cer=0.914285719 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   90.206551\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   90.261688\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "FINISHED Epoch 0023   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0024\n",
      "Training model...\n",
      "Training loss=90.110656738 avg_cer=0.899999976 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   90.082840\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   90.138474\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "Hibernating training session into directory /home/eurismar/.local/share/deepspeech\n",
      "FINISHED Epoch 0024   Overall epoch time: 0:00:01   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0025\n",
      "Training model...\n",
      "Training loss=91.127548218 avg_cer=0.914285719 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   91.062378\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   91.192719\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "FINISHED Epoch 0025   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0026\n",
      "Training model...\n",
      "Training loss=91.992828369 avg_cer=0.899999976 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   91.884491\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   92.101166\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "FINISHED Epoch 0026   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0027\n",
      "Training model...\n",
      "Training loss=91.282890320 avg_cer=0.914285719 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   91.164688\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   91.401093\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "FINISHED Epoch 0027   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0028\n",
      "Training model...\n",
      "Training loss=90.416221619 avg_cer=0.899999976 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   90.328560\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   90.503883\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "FINISHED Epoch 0028   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0029\n",
      "Training model...\n",
      "Training loss=89.398956299 avg_cer=0.914285719 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   89.347107\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   89.450798\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "Hibernating training session into directory /home/eurismar/.local/share/deepspeech\n",
      "FINISHED Epoch 0029   Overall epoch time: 0:00:01   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0030\n",
      "Training model...\n",
      "Training loss=89.137863159 avg_cer=0.928571463 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   88.989075\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   89.286644\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o\"\n",
      "FINISHED Epoch 0030   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0031\n",
      "Training model...\n",
      "Training loss=89.327011108 avg_cer=0.928571463 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   89.055450\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   89.598564\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o\"\n",
      "FINISHED Epoch 0031   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0032\n",
      "Training model...\n",
      "Training loss=89.650741577 avg_cer=0.914285719 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   89.564415\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o o\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   89.737068\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o o\"\n",
      "FINISHED Epoch 0032   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0033\n",
      "Training model...\n",
      "Training loss=88.827667236 avg_cer=0.928571463 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   88.785461\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"or\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   88.869881\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"of\"\n",
      "FINISHED Epoch 0033   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0034\n",
      "Training model...\n",
      "Training loss=88.084579468 avg_cer=0.942857146 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   88.051811\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"of\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   88.117340\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"of\"\n",
      "Hibernating training session into directory /home/eurismar/.local/share/deepspeech\n",
      "FINISHED Epoch 0034   Overall epoch time: 0:00:01   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0035\n",
      "Training model...\n",
      "Training loss=88.074981689 avg_cer=0.942857146 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   88.060783\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   88.089180\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"of\"\n",
      "FINISHED Epoch 0035   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0036\n",
      "Training model...\n",
      "Training loss=88.074188232 avg_cer=0.914285719 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   87.984245\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"too\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   88.164124\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"too\"\n",
      "FINISHED Epoch 0036   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0037\n",
      "Training model...\n",
      "Training loss=87.826599121 avg_cer=0.942857146 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   87.824203\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   87.828995\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "FINISHED Epoch 0037   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0038\n",
      "Training model...\n",
      "Training loss=86.979095459 avg_cer=0.928571463 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   86.958961\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   86.999229\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"to\"\n",
      "FINISHED Epoch 0038   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0039\n",
      "Training model...\n",
      "Training loss=86.672134399 avg_cer=0.914285719 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   86.620155\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"r\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   86.724121\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"or\"\n",
      "Hibernating training session into directory /home/eurismar/.local/share/deepspeech\n",
      "FINISHED Epoch 0039   Overall epoch time: 0:00:01   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0040\n",
      "Training model...\n",
      "Training loss=86.498474121 avg_cer=0.885714293 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   86.384041\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"or\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   86.612915\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"or\"\n",
      "FINISHED Epoch 0040   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0041\n",
      "Training model...\n",
      "Training loss=86.305587769 avg_cer=0.842857122 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   86.290581\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"r o\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   86.320587\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o r o\"\n",
      "FINISHED Epoch 0041   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0042\n",
      "Training model...\n",
      "Training loss=85.885482788 avg_cer=0.842857122 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   85.856270\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o r\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   85.914688\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o r o\"\n",
      "FINISHED Epoch 0042   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0043\n",
      "Training model...\n",
      "Training loss=84.989974976 avg_cer=0.857142866 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   84.792343\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o r\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   85.187607\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"r o\"\n",
      "FINISHED Epoch 0043   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0044\n",
      "Training model...\n",
      "Training loss=84.517990112 avg_cer=0.871428609 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   84.432983\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o r\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   84.603004\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o r\"\n",
      "Hibernating training session into directory /home/eurismar/.local/share/deepspeech\n",
      "FINISHED Epoch 0044   Overall epoch time: 0:00:01   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0045\n",
      "Training model...\n",
      "Training loss=84.583831787 avg_cer=0.885714293 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   84.397461\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o o\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   84.770195\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o r\"\n",
      "FINISHED Epoch 0045   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0046\n",
      "Training model...\n",
      "Training loss=84.163139343 avg_cer=0.885714293 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   83.984894\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o r\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   84.341385\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o\"\n",
      "FINISHED Epoch 0046   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0047\n",
      "Training model...\n",
      "Training loss=83.934616089 avg_cer=0.885714293 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   83.934044\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o r\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   83.935196\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"or\"\n",
      "FINISHED Epoch 0047   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0048\n",
      "Training model...\n",
      "Training loss=83.425842285 avg_cer=0.857142866 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   83.268127\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o r o\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   83.583557\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o o\"\n",
      "FINISHED Epoch 0048   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0049\n",
      "Training model...\n",
      "Training loss=83.343566895 avg_cer=0.828571439 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   83.241783\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o r o\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   83.445351\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o r o\"\n",
      "Hibernating training session into directory /home/eurismar/.local/share/deepspeech\n",
      "FINISHED Epoch 0049   Overall epoch time: 0:00:01   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0050\n",
      "Training model...\n",
      "Training loss=83.047927856 avg_cer=0.842857122 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   83.017914\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o pro\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   83.077942\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o r o\"\n",
      "FINISHED Epoch 0050   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0051\n",
      "Training model...\n",
      "Training loss=82.599487305 avg_cer=0.814285755 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   82.560669\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o r o\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   82.638298\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"sorry o\"\n",
      "FINISHED Epoch 0051   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0052\n",
      "Training model...\n",
      "Training loss=82.285079956 avg_cer=0.857142866 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   82.195724\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o o\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   82.374428\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o r\"\n",
      "FINISHED Epoch 0052   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0053\n",
      "Training model...\n",
      "Training loss=81.693618774 avg_cer=0.857142866 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   81.643913\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o r o\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   81.743317\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o r\"\n",
      "FINISHED Epoch 0053   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0054\n",
      "Training model...\n",
      "Training loss=81.376480103 avg_cer=0.871428609 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   81.147499\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o r\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   81.605453\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o r\"\n",
      "Hibernating training session into directory /home/eurismar/.local/share/deepspeech\n",
      "FINISHED Epoch 0054   Overall epoch time: 0:00:01   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0055\n",
      "Training model...\n",
      "Training loss=80.703445435 avg_cer=0.842857122 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   80.430702\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"oort o\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   80.976181\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o r o\"\n",
      "FINISHED Epoch 0055   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0056\n",
      "Training model...\n",
      "Training loss=80.188568115 avg_cer=0.814285755 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   79.889145\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"sorry o\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   80.487991\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"sorry\"\n",
      "FINISHED Epoch 0056   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0057\n",
      "Training model...\n",
      "Training loss=79.468025208 avg_cer=0.828571439 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   79.319130\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"oort\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   79.616920\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"o r o\"\n",
      "FINISHED Epoch 0057   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0058\n",
      "Training model...\n",
      "Training loss=78.883544922 avg_cer=0.800000012 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   78.876625\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"sorry o\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   78.890457\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"sorry o\"\n",
      "FINISHED Epoch 0058   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0059\n",
      "Training model...\n",
      "Training loss=78.592658997 avg_cer=0.828571439 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   78.514191\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"sorry\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   78.671127\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"oort\"\n",
      "Hibernating training session into directory /home/eurismar/.local/share/deepspeech\n",
      "FINISHED Epoch 0059   Overall epoch time: 0:00:01   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0060\n",
      "Training model...\n",
      "Training loss=78.221191406 avg_cer=0.828571439 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   78.065987\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"or o\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   78.376389\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"oort o\"\n",
      "FINISHED Epoch 0060   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0061\n",
      "Training model...\n",
      "Training loss=77.586349487 avg_cer=0.842857122 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   77.572571\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"oars o\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   77.600128\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"oort\"\n",
      "FINISHED Epoch 0061   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0062\n",
      "Training model...\n",
      "Training loss=76.763191223 avg_cer=0.828571439 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   76.680199\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"oorrra\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   76.846184\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"oars o\"\n",
      "FINISHED Epoch 0062   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0063\n",
      "Training model...\n",
      "Training loss=75.945121765 avg_cer=0.814285755 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   75.797012\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"carry\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   76.093231\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"carry o\"\n",
      "FINISHED Epoch 0063   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0064\n",
      "Training model...\n",
      "Training loss=75.174728394 avg_cer=0.785714269 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   75.158653\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"poorer o\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   75.190796\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"sorry o\"\n",
      "Hibernating training session into directory /home/eurismar/.local/share/deepspeech\n",
      "FINISHED Epoch 0064   Overall epoch time: 0:00:01   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0065\n",
      "Training model...\n",
      "Training loss=74.250167847 avg_cer=0.785714269 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   74.177155\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"for\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   74.323174\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"poor o\"\n",
      "FINISHED Epoch 0065   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0066\n",
      "Training model...\n",
      "Training loss=73.240112305 avg_cer=0.800000012 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   73.129311\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"for\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   73.350914\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"sorry o\"\n",
      "FINISHED Epoch 0066   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0067\n",
      "Training model...\n",
      "Training loss=72.452667236 avg_cer=0.757142901 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   72.300446\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"poor a\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   72.604881\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"door of\"\n",
      "FINISHED Epoch 0067   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0068\n",
      "Training model...\n",
      "Training loss=71.386032104 avg_cer=0.771428585 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   71.230530\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"board a\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   71.541527\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"worry a\"\n",
      "FINISHED Epoch 0068   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0069\n",
      "Training model...\n",
      "Training loss=70.712814331 avg_cer=0.757142901 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   70.556870\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"board a\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   70.868759\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"poorer a\"\n",
      "Hibernating training session into directory /home/eurismar/.local/share/deepspeech\n",
      "FINISHED Epoch 0069   Overall epoch time: 0:00:01   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0070\n",
      "Training model...\n",
      "Training loss=69.630584717 avg_cer=0.771428585 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   69.588081\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"board of\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   69.673096\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"worry a\"\n",
      "FINISHED Epoch 0070   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0071\n",
      "Training model...\n",
      "Training loss=67.990844727 avg_cer=0.771428585 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   67.887321\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"board a\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   68.094360\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"board a\"\n",
      "FINISHED Epoch 0071   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0072\n",
      "Training model...\n",
      "Training loss=66.839447021 avg_cer=0.757142901 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   66.578056\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"board a\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   67.100830\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"board of\"\n",
      "FINISHED Epoch 0072   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0073\n",
      "Training model...\n",
      "Training loss=65.172767639 avg_cer=0.757142901 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   64.663307\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"board a\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   65.682228\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"voarrr a\"\n",
      "FINISHED Epoch 0073   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0074\n",
      "Training model...\n",
      "Training loss=63.739364624 avg_cer=0.771428585 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   63.527710\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"worry a\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   63.951019\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"board a\"\n",
      "Hibernating training session into directory /home/eurismar/.local/share/deepspeech\n",
      "FINISHED Epoch 0074   Overall epoch time: 0:00:01   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0075\n",
      "Training model...\n",
      "Training loss=62.493118286 avg_cer=0.742857158 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   62.435524\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"voarrrr a\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   62.550709\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"voarrrr a\"\n",
      "FINISHED Epoch 0075   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0076\n",
      "Training model...\n",
      "Training loss=60.627880096 avg_cer=0.742857158 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   60.558735\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"poor a\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   60.697025\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"board a\"\n",
      "FINISHED Epoch 0076   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0077\n",
      "Training model...\n",
      "Training loss=59.803627014 avg_cer=0.685714304 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   59.674957\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vooarr or a\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   59.932297\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vooarrrr u a\"\n",
      "FINISHED Epoch 0077   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0078\n",
      "Training model...\n",
      "Training loss=58.436153412 avg_cer=0.685714304 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   58.231239\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vooarr or a\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   58.641068\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vaoorrrr a\"\n",
      "FINISHED Epoch 0078   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0079\n",
      "Training model...\n",
      "Training loss=56.602500916 avg_cer=0.700000048 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   56.595333\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vaoarrrr a\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   56.609673\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vaoorrrr a\"\n",
      "Hibernating training session into directory /home/eurismar/.local/share/deepspeech\n",
      "FINISHED Epoch 0079   Overall epoch time: 0:00:01   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0080\n",
      "Training model...\n",
      "Training loss=55.635757446 avg_cer=0.671428561 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   55.615356\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vaooarrr a\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   55.656158\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vapor or a\"\n",
      "FINISHED Epoch 0080   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0081\n",
      "Training model...\n",
      "Training loss=54.147674561 avg_cer=0.614285707 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   53.904472\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vapor or tuna\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   54.390873\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vaooarr or usa\"\n",
      "FINISHED Epoch 0081   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0082\n",
      "Training model...\n",
      "Training loss=53.158000946 avg_cer=0.600000024 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   52.985687\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vapor or usa\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   53.330315\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vapor or usa\"\n",
      "FINISHED Epoch 0082   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0083\n",
      "Training model...\n",
      "Training loss=51.471786499 avg_cer=0.657142878 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   51.213161\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vaorrrr usa\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   51.730412\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vaooarr or una\"\n",
      "FINISHED Epoch 0083   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0084\n",
      "Training model...\n",
      "Training loss=50.416900635 avg_cer=0.585714340 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   50.132298\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vaooarar are tuna\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   50.701508\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vaooarar or a\"\n",
      "Hibernating training session into directory /home/eurismar/.local/share/deepspeech\n",
      "FINISHED Epoch 0084   Overall epoch time: 0:00:01   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0085\n",
      "Training model...\n",
      "Training loss=48.863086700 avg_cer=0.571428597 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   48.687347\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vaoorar are una\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   49.038826\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vaoorar are una\"\n",
      "FINISHED Epoch 0085   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0086\n",
      "Training model...\n",
      "Training loss=47.729137421 avg_cer=0.542857170 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   47.517086\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vaoorar are tuna\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   47.941189\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vaoomrar are usa\"\n",
      "FINISHED Epoch 0086   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0087\n",
      "Training model...\n",
      "Training loss=46.191688538 avg_cer=0.528571486 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   45.858189\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamomrar are usa\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   46.525192\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamoomrar are usa\"\n",
      "FINISHED Epoch 0087   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0088\n",
      "Training model...\n",
      "Training loss=44.478988647 avg_cer=0.571428597 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   44.090664\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamomrar are una\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   44.867317\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamorar are a\"\n",
      "FINISHED Epoch 0088   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0089\n",
      "Training model...\n",
      "Training loss=42.917579651 avg_cer=0.514285743 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   42.828854\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamomrar are dna\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   43.006306\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamomrar are una\"\n",
      "Hibernating training session into directory /home/eurismar/.local/share/deepspeech\n",
      "FINISHED Epoch 0089   Overall epoch time: 0:00:01   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0090\n",
      "Training model...\n",
      "Training loss=41.119316101 avg_cer=0.485714316 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   41.018848\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamoomrar are una\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   41.219784\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamoomrar arruufa\"\n",
      "FINISHED Epoch 0090   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0091\n",
      "Training model...\n",
      "Training loss=39.897842407 avg_cer=0.428571433 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   39.624065\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamoomprar are fun\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   40.171619\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamoomprar are una\"\n",
      "FINISHED Epoch 0091   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0092\n",
      "Training model...\n",
      "Training loss=38.721866608 avg_cer=0.457142860 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   38.701721\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamoomrar are una\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   38.742012\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamomprar are una\"\n",
      "FINISHED Epoch 0092   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0093\n",
      "Training model...\n",
      "Training loss=37.577987671 avg_cer=0.471428573 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   37.326351\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamomprar are una\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   37.829620\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamomprar are una\"\n",
      "FINISHED Epoch 0093   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0094\n",
      "Training model...\n",
      "Training loss=36.540039062 avg_cer=0.414285719 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   36.201092\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamoomprar are gonna\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   36.878990\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vaoomprar are gonna\"\n",
      "Hibernating training session into directory /home/eurismar/.local/share/deepspeech\n",
      "FINISHED Epoch 0094   Overall epoch time: 0:00:01   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0095\n",
      "Training model...\n",
      "Training loss=34.377544403 avg_cer=0.357142866 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   34.216801\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamoomprar arrow funny\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   34.538288\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamoomprar arrow funny\"\n",
      "FINISHED Epoch 0095   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0096\n",
      "Training model...\n",
      "Training loss=33.897747040 avg_cer=0.400000006 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   33.629982\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamoomprar arrow anna\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   34.165512\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamoomprar arrow una\"\n",
      "FINISHED Epoch 0096   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0097\n",
      "Training model...\n",
      "Training loss=32.463321686 avg_cer=0.400000006 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   32.279266\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamomprar arrow funny\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   32.647377\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamomprar arrow funny\"\n",
      "FINISHED Epoch 0097   Overall epoch time: 0:00:00   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0098\n",
      "Training model...\n",
      "Training loss=30.501008987 avg_cer=0.357142866 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   30.270979\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamoomprar arroqufunna\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   30.731039\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamoomprar arrouefunna\"\n",
      "FINISHED Epoch 0098   Overall epoch time: 0:00:01   Training time: 0:00:00\n",
      "\n",
      "STARTING Epoch 0099\n",
      "Training model...\n",
      "Training loss=30.182716370 avg_cer=0.371428579 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   30.158590\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamomprar arrow funny\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   30.206841\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamoomprar arroqufunna\"\n",
      "Hibernating training session into directory /home/eurismar/.local/share/deepspeech\n",
      "FINISHED Epoch 0099   Overall epoch time: 0:00:01   Training time: 0:00:00\n",
      "\n",
      "FINISHED Optimization   Overall time: 0:02:10   Training time: 0:01:08\n",
      "\n",
      "Testing model\n",
      "Test loss=28.184822083 avg_cer=0.371428579 WER: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   28.184822\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamoomprar arroufunna\"\n",
      "--------------------------------------------------------------------------------\n",
      "    WER:    1.000000\n",
      "    loss:   28.184822\n",
      "    source: \"vamos comprar um carro que funciona\"\n",
      "    result: \"vamoomprar arroufunna\"\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ========\n",
    "\n",
    "def train():\n",
    "    r\"\"\"\n",
    "    Now, as we have prepared all the apropos operators and methods,\n",
    "    we can create the method which trains the network.\n",
    "    \"\"\"\n",
    "    print \"STARTING Optimization\\n\"\n",
    "    global_time = stopwatch()\n",
    "    global_train_time = 0\n",
    "\n",
    "    # Creating the training execution context\n",
    "    train_context = create_execution_context('train')\n",
    "\n",
    "    # Init recent word error rate levels\n",
    "    train_wer = 0.0\n",
    "    dev_wer = 0.0\n",
    "\n",
    "    hibernation_path = None\n",
    "    execution_context_running = False\n",
    "\n",
    "    # Possibly restore checkpoint\n",
    "    start_epoch = 0\n",
    "    if restore_checkpoint:\n",
    "        checkpoint = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if checkpoint and checkpoint.model_checkpoint_path:\n",
    "            hibernation_path = checkpoint.model_checkpoint_path\n",
    "            start_epoch = int(checkpoint.model_checkpoint_path.split('-')[-1])\n",
    "            print 'Resuming training from epoch %d' % (start_epoch + 1)\n",
    "\n",
    "    # Loop over the data set for training_epochs epochs\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print \"STARTING Epoch\", '%04d' % (epoch)\n",
    "\n",
    "        if epoch == 0 or hibernation_path is not None:\n",
    "            if hibernation_path is not None:\n",
    "                print \"Resuming training session from\", \"%s\" % hibernation_path, \"...\"\n",
    "            session, coord, managed_threads = start_execution_context(train_context, hibernation_path)\n",
    "            # Flag that execution context has started\n",
    "            execution_context_running = True\n",
    "        # The next loop should not load the model, unless it got set again in the meantime (by validation)\n",
    "        hibernation_path = None\n",
    "\n",
    "        overall_time = stopwatch()\n",
    "        train_time = 0\n",
    "\n",
    "        # Determine if we want to display, validate, checkpoint on this iteration\n",
    "        is_display_step = display_step > 0 and ((epoch + 1) % display_step == 0 or epoch == epochs - 1)\n",
    "        is_validation_step = validation_step > 0 and (epoch + 1) % validation_step == 0\n",
    "        is_checkpoint_step = (checkpoint_step > 0 and (epoch + 1) % checkpoint_step == 0) or epoch == epochs - 1\n",
    "\n",
    "        print \"Training model...\"\n",
    "        global_train_time = stopwatch(global_train_time)\n",
    "        train_time = stopwatch(train_time)\n",
    "        result = calculate_loss_and_report(train_context, session, epoch=epoch, query_report=is_display_step)\n",
    "        global_train_time = stopwatch(global_train_time)\n",
    "        train_time = stopwatch(train_time)\n",
    "\n",
    "        result = print_report(\"Training\", result)\n",
    "        # If there was a WER calculated, we keep it\n",
    "        if result is not None:\n",
    "            train_wer = result\n",
    "\n",
    "        # Checkpoint step (Validation also checkpoints)\n",
    "        if is_checkpoint_step and not is_validation_step:\n",
    "            print \"Hibernating training session into directory\", \"%s\" % checkpoint_dir\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, 'model.ckpt')\n",
    "            # Saving session's model into checkpoint path\n",
    "            persist_model(train_context, session, checkpoint_path, epoch)\n",
    "        # Validation step\n",
    "        if is_validation_step:\n",
    "            print \"Hibernating training session into directory\", \"%s\" % checkpoint_dir\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, 'model.ckpt')\n",
    "            # If the hibernation_path is set, the next epoch loop will load the model\n",
    "            hibernation_path = stop_execution_context(train_context, session, coord, managed_threads, checkpoint_path=checkpoint_path, global_step=epoch)\n",
    "            # Flag that execution context has stoped\n",
    "            execution_context_running = False\n",
    "\n",
    "            # Validating the model in a fresh session\n",
    "            print \"Validating model...\"\n",
    "            result = run_set('dev', model_path=hibernation_path, query_report=True)\n",
    "            result = print_report(\"Validation\", result)\n",
    "            # If there was a WER calculated, we keep it\n",
    "            if result is not None:\n",
    "                dev_wer = result\n",
    "\n",
    "        overall_time = stopwatch(overall_time)\n",
    "\n",
    "        print \"FINISHED Epoch\", '%04d' % (epoch),\\\n",
    "              \"  Overall epoch time:\", format_duration(overall_time),\\\n",
    "              \"  Training time:\", format_duration(train_time)\n",
    "        print\n",
    "\n",
    "    # If the last iteration step was no validation, we still have to save the model\n",
    "    if hibernation_path is None or execution_context_running:\n",
    "        hibernation_path = stop_execution_context(train_context, session, coord, managed_threads, checkpoint_path=checkpoint_path, global_step=epoch)\n",
    "\n",
    "    # Indicate optimization has concluded\n",
    "    print \"FINISHED Optimization\",\\\n",
    "          \"  Overall time:\", format_duration(stopwatch(global_time)),\\\n",
    "          \"  Training time:\", format_duration(global_train_time)\n",
    "    print\n",
    "\n",
    "    return train_wer, dev_wer, hibernation_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # As everything is prepared, we are now able to do the training.\n",
    "    # Define CPU as device on which the muti-gpu training is orchestrated\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Take start time for time measurement\n",
    "        time_started = datetime.datetime.utcnow()\n",
    "\n",
    "        # Train the network\n",
    "        last_train_wer, last_dev_wer, hibernation_path = train()\n",
    "\n",
    "        # Take final time for time measurement\n",
    "        time_finished = datetime.datetime.utcnow()\n",
    "\n",
    "        # Calculate duration in seconds\n",
    "        duration = time_finished - time_started\n",
    "        duration = duration.days * 86400 + duration.seconds\n",
    "\n",
    "        # Finally the model is tested against some unbiased data-set\n",
    "        print \"Testing model\"\n",
    "        result = run_set('test', model_path=hibernation_path, query_report=True)\n",
    "        test_wer = print_report(\"Test\", result)\n",
    "\n",
    "\n",
    "    # Finally, we restore the trained variables into a simpler graph that we can export for serving.\n",
    "    # Don't export a model if no export directory has been set\n",
    "    if export_dir:\n",
    "        with tf.device('/cpu:0'):\n",
    "            tf.reset_default_graph()\n",
    "            session = tf.Session(config=session_config)\n",
    "\n",
    "            # Run inference\n",
    "\n",
    "            # Input tensor will be of shape [batch_size, n_steps, n_input + 2*n_input*n_context]\n",
    "            input_tensor = tf.placeholder(tf.float32, [None, None, n_input + 2*n_input*n_context], name='input_node')\n",
    "\n",
    "            # Calculate input sequence length. This is done by tiling n_steps, batch_size times.\n",
    "            # If there are multiple sequences, it is assumed they are padded with zeros to be of\n",
    "            # the same length.\n",
    "            n_items  = tf.slice(tf.shape(input_tensor), [0], [1])\n",
    "            n_steps = tf.slice(tf.shape(input_tensor), [1], [1])\n",
    "            seq_length = tf.tile(n_steps, n_items)\n",
    "\n",
    "            # Calculate the logits of the batch using BiRNN\n",
    "            logits = BiRNN(input_tensor, tf.to_int64(seq_length), no_dropout)\n",
    "\n",
    "            # Beam search decode the batch\n",
    "            decoded, _ = tf.nn.ctc_beam_search_decoder(logits, seq_length, merge_repeated=False)\n",
    "            decoded = tf.convert_to_tensor(\n",
    "                [tf.sparse_tensor_to_dense(sparse_tensor) for sparse_tensor in decoded], name='output_node')\n",
    "\n",
    "            # TODO: Transform the decoded output to a string\n",
    "\n",
    "            # Create a saver and exporter using variables from the above newly created graph\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            model_exporter = exporter.Exporter(saver)\n",
    "\n",
    "            # Restore variables from training checkpoint\n",
    "            # TODO: This restores the most recent checkpoint, but if we use validation to counterract\n",
    "            #       over-fitting, we may want to restore an earlier checkpoint.\n",
    "            checkpoint = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "            checkpoint_path = checkpoint.model_checkpoint_path\n",
    "            saver.restore(session, checkpoint_path)\n",
    "            print 'Restored checkpoint at training epoch %d' % (int(checkpoint_path.split('-')[-1]) + 1)\n",
    "\n",
    "            # Initialise the model exporter and export the model\n",
    "            model_exporter.init(session.graph.as_graph_def(),\n",
    "                                named_graph_signatures = {\n",
    "                                    'inputs': exporter.generic_signature(\n",
    "                                        { 'input': input_tensor }),\n",
    "                                    'outputs': exporter.generic_signature(\n",
    "                                        { 'outputs': decoded})})\n",
    "            if remove_export:\n",
    "                actual_export_dir = os.path.join(export_dir, '%08d' % export_version)\n",
    "                if os.path.isdir(actual_export_dir):\n",
    "                    print 'Removing old export'\n",
    "                    shutil.rmtree(actual_export_dir)\n",
    "            try:\n",
    "                # Export serving model\n",
    "                model_exporter.export(export_dir, tf.constant(export_version), session)\n",
    "\n",
    "                # Export graph\n",
    "                input_graph_name = 'input_graph.pb'\n",
    "                tf.train.write_graph(session.graph, export_dir, input_graph_name, as_text=False)\n",
    "\n",
    "                # Freeze graph\n",
    "                input_graph_path = os.path.join(export_dir, input_graph_name)\n",
    "                input_saver_def_path = ''\n",
    "                input_binary = True\n",
    "                output_node_names = 'output_node'\n",
    "                restore_op_name = 'save/restore_all'\n",
    "                filename_tensor_name = 'save/Const:0'\n",
    "                output_graph_path = os.path.join(export_dir, 'output_graph.pb')\n",
    "                clear_devices = False\n",
    "                freeze_graph.freeze_graph(input_graph_path, input_saver_def_path,\n",
    "                                          input_binary, checkpoint_path, output_node_names,\n",
    "                                          restore_op_name, filename_tensor_name,\n",
    "                                          output_graph_path, clear_devices, '')\n",
    "\n",
    "                print 'Models exported at %s' % (export_dir)\n",
    "            except RuntimeError:\n",
    "                print  sys.exc_info()[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-68-5e5fb4c9eb20>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-68-5e5fb4c9eb20>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    data_sets = read_data_sets([\"train\", \"dev\", \"test\"])\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    " # Logging Hyper Parameters and Results\n",
    "    # ====================================\n",
    "\n",
    "    # Now, as training and test are done, we persist the results alongside\n",
    "    # with the involved hyper parameters for further reporting.\n",
    "    data_sets = read_data_sets([\"train\", \"dev\", \"test\"])\n",
    "\n",
    "    with open('%s/%s' % (log_dir, 'hyper.json'), 'w') as dump_file:\n",
    "        json.dump({\n",
    "            'context': {\n",
    "                'time_started': time_started.isoformat(),\n",
    "                'time_finished': time_finished.isoformat(),\n",
    "                'git_hash': get_git_revision_hash(),\n",
    "                'git_branch': get_git_branch()\n",
    "            },\n",
    "            'parameters': {\n",
    "                'learning_rate': learning_rate,\n",
    "                'beta1': beta1,\n",
    "                'beta2': beta2,\n",
    "                'epsilon': epsilon,\n",
    "                'epochs': epochs,\n",
    "                'train_batch_size': train_batch_size,\n",
    "                'dev_batch_size': dev_batch_size,\n",
    "                'test_batch_size': test_batch_size,\n",
    "                'validation_step': validation_step,\n",
    "                'dropout_rates': dropout_rates,\n",
    "                'relu_clip': relu_clip,\n",
    "                'n_input': n_input,\n",
    "                'n_context': n_context,\n",
    "                'n_hidden_1': n_hidden_1,\n",
    "                'n_hidden_2': n_hidden_2,\n",
    "                'n_hidden_3': n_hidden_3,\n",
    "                'n_hidden_5': n_hidden_5,\n",
    "                'n_hidden_6': n_hidden_6,\n",
    "                'n_cell_dim': n_cell_dim,\n",
    "                'n_character': n_character,\n",
    "                'total_batches_train': data_sets.train.total_batches,\n",
    "                'total_batches_validation': data_sets.dev.total_batches,\n",
    "                'total_batches_test': data_sets.test.total_batches,\n",
    "                'data_set': {\n",
    "                    'name': ds_importer\n",
    "                }\n",
    "            },\n",
    "            'results': {\n",
    "                'duration': duration,\n",
    "                'last_train_wer': last_train_wer,\n",
    "                'last_validation_wer': last_dev_wer,\n",
    "                'test_wer': test_wer\n",
    "            }\n",
    "        }, dump_file, sort_keys=True, indent=4)\n",
    "\n",
    "    # Let's also re-populate a central JS file, that contains all the dumps at once.\n",
    "    merge_logs(logs_dir)\n",
    "    maybe_publish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "last_train_wer, last_dev_wer, hibernation_path = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
